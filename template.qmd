---
title: "Project"
subtitle: "Precipitation Downscaling"
jupyter: julia-1.9
date: 2023-11-13
author: "Kyle Ostlind(ko23)" 

number-sections: true
code-annotations: hover
echo: false

kind: "Lab"
Module: "2"
categories:
    - "Module 2"
    - "Labs"

format:
    html: 
        toc-depth: 3
    docx: 
        toc: true
        toc-depth: 3
        fig-format: png
---


## executive summary

For my project I decided to attempt to use pressure data retrieved from the ERA5 reanalysis dataset and NEXRAD precipitation data to generate precipitation fields over southeast Texas. Twenty years of pressure data were used in this analysis, from 2000 to 2022. To accomplish this, I chose to use Principal component analysis to reduce the dimensions, and then a K nearest neighbor algorithm to generate predictions by sampling from the precipitation data set. Looking to improve the generated precipitation fields, I implemented a quantile-quantile mapping scheme. Where the output from the PCA-KNN is adjusted by matching corresponding quantiles from the actual observations. This would help the prediction better match the actual precipitation fields. 

Principal Component Analysis is a technique that is used to analyze high dimensional systems, where the high dimensionality would otherwise be less easy to interpret and preform analysis on. It is accomplished by transforming the data into a new coordinate system that consists of fewer axes, or dimensions, than the original dataset. These new coordinates, called principal components, are chosen such that they explain the most variance in the predictors. Thus retaining the explaining power of the original variables. K-Nearest-Neighbors, is a prediction technique that assumes the value of a piece of data is related to the values that are near to it. A prediction is made by averaging the neighbors or randomly sampling from them. The second method I have chosen to implement is quantile-quantile mapping. This method is used to eliminate biases in data. These biases can come from coarse data, or poor model design. It is performed by mapping the predicted values quantile to the quantiles of the observed data. Essentially trying to have the predicted data's CDF equal the observed data's CDF.


## exploratory data analysis

```{julia}
#| output: false
using Dates
using MultivariateStats
using Plots
using NCDatasets
using StatsBase
using Unitful
using Distances
using Metrics
```



```{julia}
#read in precipitation data in to NCDataset object
precip_ds = NCDataset("data/precip_tx.nc")
precip_time = precip_ds["time"][:]
precip_lon = precip_ds["lon"][:]
precip_lat = precip_ds["lat"][:]
precip = precip_ds["precip"][:, :, :];
```

```{julia}
precip_lat = reverse(precip_lat)
precip = reverse(precip; dims=2)

precip = reverse(precip; dims=2)
#precip = replace(precip, missing=>NaN)
#display(precip_ds[:precip]) # learn about it
close(precip_ds);

```

```{julia}
heatmap(
precip_lon,
precip_lat,
precip[:, :, 2]';
xlabel="Longitude",
ylabel="Latitude",
title="Precipitation on $(precip_time[1])"
)
```
```{julia}
#trim precip data to fit temp data size 
idx = findfirst(precip_time .== Dates.Date("2000-01-01"))
idx2 = findfirst(precip_time .== Dates.Date("2022-12-31"))
precip_time_cut = precip_time[idx:idx2];
```

```{julia}
function open_mfdataset(files::Vector{String}, variable_name::AbstractString)
    # Lists to store variable data, time data, and other coordinate data
    var_data_list = []
    time_data_list = []
    coords_data_dict = Dict()

    # Open the first file to get the coordinate names (excluding time and the main variable)
    ds = Dataset(files[1])
    dimnames = keys(ds.dim)
    coord_names = setdiff(collect(dimnames), [variable_name, "time"])
    close(ds)

    # Initialize lists for each coordinate in coords_data_dict
    for coord in coord_names
        coords_data_dict[coord] = []
    end

    # Open each file, extract data, and store in lists
    for file in files
        ds = Dataset(file)

        # Store variable and time data
        push!(var_data_list, ds[variable_name][:])
        push!(time_data_list, ds["time"][:])

        # Store other coordinate data
        for coord in coord_names
            push!(coords_data_dict[coord], ds[coord][:])
        end

        close(ds)
    end

    # Pair variable data with time data and sort by time
    sorted_pairs = sort(collect(zip(time_data_list, var_data_list)); by=x -> x[1])
    sorted_time_data = [pair[1] for pair in sorted_pairs]
    sorted_var_data = [pair[2] for pair in sorted_pairs]

    # Concatenate sorted data
    concatenated_data_dict = Dict(
        variable_name => vcat(sorted_var_data...), "time" => vcat(sorted_time_data...)
    )

    # Concatenate coordinate data and add to the dictionary
    for coord in coord_names
        concatenated_data_dict[coord] = vcat(coords_data_dict[coord]...)
    end

    return concatenated_data_dict
end
```

```{julia}
#open files and extract temp and pressure data into NCdataset and/or Dictionaries 
data_dict_temp = open_mfdataset(["data/raw/2m_temperature_2000.nc", "data/raw/2m_temperature_2001.nc", "data/raw/2m_temperature_2002.nc","data/raw/2m_temperature_2003.nc","data/raw/2m_temperature_2004.nc","data/raw/2m_temperature_2005.nc","data/raw/2m_temperature_2006.nc","data/raw/2m_temperature_2007.nc","data/raw/2m_temperature_2008.nc","data/raw/2m_temperature_2009.nc","data/raw/2m_temperature_2010.nc","data/raw/2m_temperature_2011.nc","data/raw/2m_temperature_2012.nc","data/raw/2m_temperature_2013.nc","data/raw/2m_temperature_2014.nc","data/raw/2m_temperature_2015.nc","data/raw/2m_temperature_2016.nc","data/raw/2m_temperature_2017.nc","data/raw/2m_temperature_2018.nc","data/raw/2m_temperature_2019.nc","data/raw/2m_temperature_2020.nc","data/raw/2m_temperature_2021.nc","data/raw/2m_temperature_2022.nc"], "t2m")

# println(length(data_dict_temp[:"time"]))
# println(length(data_dict_temp[:"t2m"]))

temp_ds = NCDataset("data/raw/2m_temperature_2019.nc")

data_dict_press = open_mfdataset(["data/raw/500hPa_geopotential_2000.nc", "data/raw/500hPa_geopotential_2001.nc", "data/raw/500hPa_geopotential_2002.nc","data/raw/500hPa_geopotential_2003.nc","data/raw/500hPa_geopotential_2004.nc","data/raw/500hPa_geopotential_2005.nc","data/raw/500hPa_geopotential_2006.nc","data/raw/500hPa_geopotential_2007.nc","data/raw/500hPa_geopotential_2008.nc","data/raw/500hPa_geopotential_2009.nc","data/raw/500hPa_geopotential_2010.nc","data/raw/500hPa_geopotential_2011.nc","data/raw/500hPa_geopotential_2012.nc","data/raw/500hPa_geopotential_2013.nc","data/raw/500hPa_geopotential_2014.nc","data/raw/500hPa_geopotential_2015.nc","data/raw/500hPa_geopotential_2016.nc","data/raw/500hPa_geopotential_2017.nc","data/raw/500hPa_geopotential_2018.nc","data/raw/500hPa_geopotential_2019.nc","data/raw/500hPa_geopotential_2020.nc","data/raw/500hPa_geopotential_2021.nc","data/raw/500hPa_geopotential_2022.nc"], "z");
```

```{julia}
#reshape and correct data 

dataTemp = reshape(data_dict_temp[:"t2m"],13, 13,length(data_dict_temp[:"time"]))
dataPress = reshape(data_dict_press[:"z"],13, 13,length(data_dict_press[:"time"]))

temp_lat = reverse(temp_ds[:"latitude"])
temp_lon = temp_ds[:"longitude"]
temp = reverse(dataTemp; dims=2)

temp_time = data_dict_temp[:"time"]
press_time = data_dict_press[:"time"]

# #check the form of the data 
temp_data_plot = heatmap(
temp_lon,
temp_lat,
dataTemp[:, :, 1]';
xlabel="Longitude",
ylabel="Latitude",
title="Temperature - hour 1"
)

display(temp_data_plot)

press_data_plot = heatmap(
temp_lon,
temp_lat,
dataPress[:, :, 1]';
xlabel="Longitude",
ylabel="Latitude",
title="Pressure - hour 1"
)

```



plots of temperature and pressure to make sure the data looks right. It appears there is less gradient in the pressure data compared to the temperature across the land ocean boundary. 



```{julia}
#average by day to match the precip data 
temp_byDay = reshape(dataTemp, length(temp_lat), length(temp_lat), 24, 8401)
daily_mean = mean(temp_byDay, dims=3)
temp_byDay = reshape(daily_mean,length(temp_lat),length(temp_lat),8401)

#same for pressure
press_byDay = reshape(dataPress, length(temp_lat), length(temp_lat), 24, 8401)
daily_mean_press = mean(press_byDay, dims=3)
press_byDay = reshape(daily_mean_press,length(temp_lat),length(temp_lat),8401);
```


```{julia}
#check time and convert to Date data type 
@assert temp_time == press_time
time1 = Dates.Date.(temp_time)
time_p = Dates.Date.(precip_time_cut);
```

```{julia}
#split the data 5544 days from end of data, roughly 2/3 split

idx_partition = findfirst(time_p .== time_p[end] - Dates.Day(5544))
train_idx = 1:idx_partition
test_idx = (idx_partition+1):length(time_p)
precip_train = precip[:, :, train_idx]
precip_test = precip[:, :, test_idx]
temp_train = temp_byDay[:, :, train_idx]
temp_test = temp_byDay[:, :, test_idx]
press_train = press_byDay[:, :, train_idx]
press_test = press_byDay[:, :, test_idx];

```

```{julia}
function preprocess(temp::Array{T,3}, temp_ref::Array{T,3})::AbstractMatrix where {T}
    n_lon, n_lat, n_t = size(temp)
    climatology = mean(temp_ref; dims=3)
    temp_anom = temp .- climatology
    # reshape to 2D
    temp_anom = reshape(temp_anom, n_lon * n_lat, n_t)
    # strip the units
    return temp_anom
end
```

```{julia}
n_lon, n_lat, n_t = size(temp)
temp_mat_train = preprocess(temp_train, temp_train)
temp_mat_test = preprocess(temp_test, temp_train)
press_mat_train = preprocess(press_train, press_train)
press_mat_test = preprocess(press_test, press_train)

data_train = vcat(temp_mat_train,press_mat_train);
```

```{julia}
#fit PCA to train dataset 
pca_model = fit(PCA, press_mat_train; maxoutdim=25, pratio=0.999);
```

```{julia}

p1 = plot(
principalvars(pca_model) / var(pca_model);
xlabel="# of PCs",
ylabel="Fraction of Variance Explained",
xticks = 0:2:25,
label=false,
title="Variance Explained"
)
p2 = plot(
cumsum(principalvars(pca_model)) / var(pca_model);
xlabel="# of PCs",
ylabel="Fraction of Variance Explained",
xticks = 0:2:25,
label=false,
title="Cumulative Variance Explained Plot"
)
plot(p1, p2; layout=(1, 2), size=(800, 400))

```

Plots of the variance explained and cumulative variance for each aditional PCA axis. 

```{julia}
p = []
for i in 1:3
pc = projection(pca_model)[:, i]
pc = reshape(pc, n_lat, n_lon)'
pi = heatmap(
temp_lon,
temp_lat,
pc;
xlabel="Longitude",
ylabel="Latitude",
title="PC $i",
aspect_ratio=:equal,
cmap=:PuOr
)
push!(p, pi)
end
plot(p...; layout=(1, 3), size=(1500, 600))

```

Plot of the spatial dependences of the PCA axes. 

```{julia}
pc_ts = predict(pca_model, press_mat_train)
day_of_year = Dates.dayofyear.(time1)
p = []
for i in 1:3
pi = scatter(
day_of_year,
pc_ts[i, :];
xlabel="Day of Year",
ylabel="PC $i",
title="PC $i",
label=false,
alpha=0.3,
color=:gray
)
push!(p, pi)
end
#plot(p...; layout=(1, 3), size=(1500, 600))
```

```{julia}
avg_precip = [mean(skipmissing(precip_train[:, :, t])) for t in 1:size(precip_train, 3)]
avg_precip = replace(avg_precip, NaN => 0)
p1 = scatter(
pc_ts[2, :],
pc_ts[3, :];
zcolor=avg_precip,
xlabel="PC 2",
ylabel="PC 3",
markersize=3,
clims=(0, 2.75),
title="All Days",
label=false
)
p2_idx = findall(avg_precip .> quantile(avg_precip, 0.98))
p2 = scatter(
pc_ts[2, p2_idx],
pc_ts[3, p2_idx];
zcolor=avg_precip[p2_idx],
xlabel="PC 2",
ylabel="PC 3",
markersize=5,
clims=(0, 2.75),
title="High Pressure Days",
label=false
)
plot(p1, p2; size=(1000, 400), link=:both)

```

principal components plotted against eachother, no clear trend in the plots.

```{julia}
function euclidean_distance(x::AbstractVector, y::AbstractVector)::AbstractFloat
    return sqrt(sum((x .- y) .^ 2))
end
function nsmallest(x::AbstractVector, n::Int)::Vector{Int}
    idx = sortperm(x)
    return idx[1:n]
end
function knn(X::AbstractMatrix, X_i::AbstractVector, K::Int)::Tuple{Int,AbstractVector}
# calculate the distances between X_i and each row of X
    dist = [euclidean_distance(X_i, X[j, :]) for j in 1:size(X, 1)]
    idx = nsmallest(dist, K)
    w = 1 ./ dist[idx]
    w ./= sum(w)
    idx_sample = sample(idx, Weights(w))
    return (idx_sample, vec(X[idx_sample, :]))
end
```

```{julia}
function predict_knn(temp_train, temp_test, precip_train; n_pca::Int)
    X_train = preprocess(temp_train, temp_train)
    X_test = preprocess(temp_test, temp_train)
    # fit the PCA model to the training data
    pca_model = fit(PCA, X_train; maxoutdim=n_pca)
    # project the test data onto the PCA basis
    train_embedded = predict(pca_model, X_train)
    test_embedded = predict(pca_model, X_test)
    # use the `knn` function for each point in the test data
    precip_pred = map(1:size(X_test, 2)) do i
    idx, _ = knn(train_embedded', test_embedded[:, i], 2)
    precip_train[:, :, idx]
end
# return a matrix of predictions
return precip_pred
end
```

```{julia}
    t_sample = rand(1:size(press_test, 3), 3)
    #t_sample = [1200,500,2300]
    precip_pred = predict_knn(press_train, press_test[:, :, t_sample], precip_train; n_pca=3)
    p = map(eachindex(t_sample)) do ti
    t = t_sample[ti]
    y_pred = precip_pred[ti]'
    y_actual = precip_test[:, :, t]'
    
    cmax = max(maximum(skipmissing(y_pred)), maximum(skipmissing(y_actual)))
    p1 = heatmap(
    precip_lon,
    precip_lat,
    y_pred;
    xlabel="Longitude",
    ylabel="Latitude",
    title="Predicted",
    xticks = 250:5:270,
    aspect_ratio=:equal,
    clims=(0, cmax)
    )
    
    
    y = round(abs(sum(skipmissing(y_pred - y_actual))/576); digits=3)

    p2 = heatmap(
    precip_lon,
    precip_lat,
    y_actual;
    xlabel="Longitude",
    ylabel="Latitude",
    title="Actual\nMAE="*"$y",
    xticks = 250:5:270,
    aspect_ratio=:equal,
    clims=(0, cmax)
    )

    plot(p1, p2; layout=(2, 1), size=(1000, 400))
    end

    plot(p...; layout=(2, 3), size=(1300, 800))

```

```{julia}
precip_pred_QQ = predict_knn(press_train, press_test[:, :, [1800,500,2100,750,100,1300]], precip_train; n_pca=3);
```

```{julia}
#quantile-quantile mapping
y_pred_QQ = precip_pred_QQ[2]'
#display(y_pred_QQ)
y_actual_QQ = precip_test[:, :, 500]
y_pred_QQ = reshape(y_pred_QQ,length(precip_lon)*length(precip_lat))
y_pred_corrected = zeros((length(precip_lon), length(precip_lat)))

for i in skipmissing(y_pred_QQ)
        #Chat GPT generated code to calcute the quantile of a datapoint
        point_quant = searchsortedfirst(sort(y_pred_QQ), i) / length(y_pred_QQ)
        #calculate the corresponding value at the quantile of the predicted datapoint
        quant_actual = quantile(skipmissing(y_actual_QQ),point_quant)
        idx = findfirst(x -> x == i, skipmissing(y_pred_QQ))
        #replace value with the mapped value
        y_pred_corrected[idx] = quant_actual

end
y_pred_corrected = reshape(y_pred_corrected,length(precip_lon), length(precip_lat));
```

```{julia}
#quantile-quantile mapping
y_pred_QQ_1 = precip_pred_QQ[6]'
y_actual_QQ_1 = precip_test[:, :, 1300]
y_pred_QQ_1 = reshape(y_pred_QQ_1,length(precip_lon)*length(precip_lat))
y_pred_corrected_1 = zeros((length(precip_lon), length(precip_lat)))

for i in skipmissing(y_pred_QQ_1)
        #Chat GPT generated code to calcute the quantile of a datapoint
        point_quant = searchsortedfirst(sort(y_pred_QQ_1), i) / length(y_pred_QQ_1)
        #calculate the corresponding value at the quantile of the predicted datapoint
        quant_actual = quantile(skipmissing(y_actual_QQ_1),point_quant)
        idx = findfirst(x -> x == i, skipmissing(y_pred_QQ_1))
        #replace value with the mapped value
        y_pred_corrected_1[idx] = quant_actual

end
y_pred_corrected_1 = reshape(y_pred_corrected_1,length(precip_lon), length(precip_lat));
```


```{julia}
y_pred_QQ = precip_pred_QQ[2]'
y_pred_QQ_1 = precip_pred_QQ[6]'

#mean absolute error
y = round(abs(sum(skipmissing(y_pred_corrected - y_actual_QQ))/576); digits=3)
x = round(abs(sum(skipmissing(y_pred_QQ - y_actual_QQ))/576);digits=3)
y1 = round(abs(sum(skipmissing(y_pred_corrected_1 - y_actual_QQ_1))/576); digits=3)
x1 = round(abs(sum(skipmissing(y_pred_QQ_1 - y_actual_QQ_1))/576);digits=3)

plot1 = heatmap(
precip_lon,
precip_lat,
y_pred_corrected;
xlabel="Longitude",
ylabel="Latitude",
title="PCA-KNN corrected:\nMAE="*"$y",
xticks = 250:5:270,
aspect_ratio=:equal,
)

plot2 = heatmap(
precip_lon,
precip_lat,
y_pred_QQ;
xlabel="Longitude",
ylabel="Latitude",
title="PCA-KNN predicted: \nMAE="*"$x",
xticks = 250:5:270,
aspect_ratio=:equal,
)

plot3 = heatmap(
precip_lon,
precip_lat,
y_actual_QQ';
xlabel="Longitude",
ylabel="Latitude",
title="Actual",
xticks = 250:5:270,
aspect_ratio=:equal,
)

plot4 = heatmap(
precip_lon,
precip_lat,
y_pred_corrected_1;
xlabel="Longitude",
ylabel="Latitude",
title="PCA-KNN corrected:\nMAE="*"$y",
xticks = 250:5:270,
aspect_ratio=:equal,
)

plot5 = heatmap(
precip_lon,
precip_lat,
y_pred_QQ_1;
xlabel="Longitude",
ylabel="Latitude",
title="PCA-KNN predicted: \nMAE="*"$x",
xticks = 250:5:270,
aspect_ratio=:equal,
)

plot6 = heatmap(
precip_lon,
precip_lat,
y_actual_QQ_1';
xlabel="Longitude",
ylabel="Latitude",
title="Actual",
xticks = 250:5:270,
aspect_ratio=:equal,
)

plot(plot1,plot2,plot3,plot4,plot5,plot6; layout=(2, 3),size=(800, 400))

```
Plots of the QQ correted precipitation fields with the uncorrected PCA-KNN fields and the actual observed fields. The top row is for timestep 500, and the bottom is timestep 1300.


## methods
Data: The first decision to be made was how much data to include in the analysis. As I am looking to make predictions about the daily precipitation from regional pressure data, a longer period to capture the full climatology is appropriate. For this project I chose 22 years of data from the ERA5 reanalysis dataset. The pressure data comes as hourly data, where the precipitation fields come on a daily timescale. To make these two datasets match, I took a daily mean of all the hourly pressure data. 

PCA: The next step was to split the data into a training set and a testing set. Two thirds went to the training sets, and the remaining third went to the testing sets. When conducting PCA analysis it is important to standardize the data so the size of one variable compared to another doesn’t dominate the variance. To accomplish this the mean was subtracted from the data. To determine how many PCA axes was necessary a plot of the variance explained per axis was made, from this I determined three PCA axes was sufficient. 

KNN: In the K-Nearest-Neighbors algorithm, I chose to use a simple Euclidean distance to calculate the nearest neighbors. This is appropriate, as our main variable in the PCA analysis is pressure which is a smooth and continuous variable. To make the prediction, a random weighted sample of the neighbors was made. The samples were weighted by distance, so the closer neighbors were to each other, the more likely they were to be chosen. To determine the amount of neighbors to sample I experimented by changing the number of neighbors and visually assessed the impact on the predictions. This led me to believe that two neighbors were a decent choice.

Q-Q Mapping: To implement my quantile mapping correction, I went by simply comparing the quantiles of the predicted and observed precipitation fields. To validate the effectiveness of my model I did a simple mean absolute error calculation between the corrected and the uncorrected predictions vs the observed precipitation field. The goal of Q-Q mapping is to adjust the CDF of the predicted values to the CDF of the observed values. A better way to validate would be to calculate an empirical CDF for each of the predicted precipitation fields and plot them against the observed ECDF. This would tell us how successful we were in adjusting the predicted CDF to match the observed. I tried to implement this but could not resolve some errors in my code. 

## model comparison 

Plotting the PCA-KNN predictions against the actual precipitation fields and just visually we can see that there are some large discrepancies. This is confirmed by a mean absolute error value. The mean absolute error is a reasonable metric as it penalizes larger error, rewarding predictions that match the spatial and intensive properties of the observed fields. Another observation is that the PCA-KNN analysis appears to not match the intensity of the precipitation fields in most cases. Where there is either too little or too much rain. It also doesn't capture the spatial organization of the field very well either. These issues could be a symptom of the number of nearest neighbors being too low. Another issue with the model is that only pressure is used thus we may not be capturing the full system. Accounting for more climate variables such as temperature and water vapor would likely help the overall prediction. 

The second model, with the addition of a Quantile-Quantile mapping step after the PCA-KNN analysis, seems to perform slightly better. It is most improved in its ability to match the total amount of rainfall on the grid. This makes sense as we are directly mapping from the precipitation values of each day. Though, it does have a tendency to way over or under predict the amount of local precipitation in certain areas. Where if the uncorrected prediction field has more precipitation coverage over the grid, and the observed has high intensities. It will map to those higher intensity quantiles more often. The dependence on the agreement of the underlying PCA-KNN model is a major limitation. Preforming a Q-Q mapping analysis on just the pressure or temperature and the precipitation data could make it less susceptible to these swings. This model performed just about as poorly as the uncorrected PCA-KNN in its ability to predict the spatial organization of the observed fields. This is expected as the predictions are built off the underlying PCA-KNN model. 

## conclusion
I explored the prediction power of a PCA-KNN model, compared with a similar model with an extra correction step from a Quantile-Quantile mapping scheme. I found that the PCA-KNN model does a poor job of matching both the intensity and spatial distribution of the rainfall field. This result may be attributed to the lack of nearest neighbors considered and not including more climate variables. The addition of the Quantile-Quantile mapping helped improve the model slightly, representing the total rainfall more accurately. Though it did seem more susceptible to extreme swings in its predictions. A result of the dependence on the model it is trying to correct.  
 