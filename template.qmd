---
title: "Project"
subtitle: "Precipitation Downscaling"
jupyter: julia-1.9
date: 2023-11-13
author: "Kyle Ostlind(ko23)" # UNCOMMENT AND ADD YOUR NAME

number-sections: true
code-annotations: hover
echo: false

kind: "Lab"
Module: "2"
categories:
    - "Module 2"
    - "Labs"

format:
    html: 
        toc-depth: 3
    docx: 
        toc: true
        toc-depth: 3
        fig-format: png
---


## executive summary

For my project I decided to attempt to use pressure data retrieved from the ERA5 reanalysis dataset and NEXRAD precipitation data to generate precipitation fields over south east Texas. Twenty years of pressure data were used in this analysis, from 2000 to 2022. To accomplish this I chose to use Principal component analysis to reduce the dimensions, and then a K nearest neighbor algorithm to generate predictions by sampling from the precipitation data set. Looking to improve the generated precipitation fields, I implemented a quantile-quantile mapping scheme. Where the output from the PCA-KNN is adjusted by matching corresponding quartiles from the actual observations. This would help the prediction better match the actual precipitation fields. 

Principal Component Analysis is a technique that is used to analyze high dimensional systems, where the high dimensionality would otherwise be less easy to interpret and preform analysis on. It is accomplished by transforming the data into a new coordinate system that consists of fewer axes, or dimensions,  than the original dataset. These new coordinates, called principal components, are chosen such that they explain the most variance in the predictors. Thus retaining the explaining power of the original variables.K-Nearest-Neighbors, is a prediction technique that assumes the value of a piece of data is related to the values that are near to it. A prediction is made by averaging the neighbors or randomly sampling from them. The second method I have chosen to implement is quantile-quantile mapping. This method is used to eliminate biases in data. These biases can come from coarse data, or poor model design. It is preformed by mapping the predicted values quantile to the quantiles of the observed data. Essentially trying to have the predicted data's CDF equal the observed data's CDF. 

## exploratory data analysis

```{julia}
#| output: false
using Dates
using MultivariateStats
using Plots
using NCDatasets
using StatsBase
using Unitful
using Distances
using Metrics
```



```{julia}
precip_ds = NCDataset("data/precip_tx.nc")
precip_time = precip_ds["time"][:]
precip_lon = precip_ds["lon"][:]
precip_lat = precip_ds["lat"][:]
precip = precip_ds["precip"][:, :, :];
```

```{julia}
precip_lat = reverse(precip_lat)
precip = reverse(precip; dims=2)

precip = reverse(precip; dims=2)
#precip = replace(precip, missing=>NaN)
#display(precip_ds[:precip]) # learn about it
close(precip_ds);

```

```{julia}
heatmap(
precip_lon,
precip_lat,
precip[:, :, 2]';
xlabel="Longitude",
ylabel="Latitude",
title="Precipitation on $(precip_time[1])"
)
```
```{julia}
#cut down precip data to fit temp data size 
idx = findfirst(precip_time .== Dates.Date("2000-01-01"))
idx2 = findfirst(precip_time .== Dates.Date("2022-12-31"))
precip_time_cut = precip_time[idx:idx2];
```

```{julia}
function open_mfdataset(files::Vector{String}, variable_name::AbstractString)
    # Lists to store variable data, time data, and other coordinate data
    var_data_list = []
    time_data_list = []
    coords_data_dict = Dict()

    # Open the first file to get the coordinate names (excluding time and the main variable)
    ds = Dataset(files[1])
    dimnames = keys(ds.dim)
    coord_names = setdiff(collect(dimnames), [variable_name, "time"])
    close(ds)

    # Initialize lists for each coordinate in coords_data_dict
    for coord in coord_names
        coords_data_dict[coord] = []
    end

    # Open each file, extract data, and store in lists
    for file in files
        ds = Dataset(file)

        # Store variable and time data
        push!(var_data_list, ds[variable_name][:])
        push!(time_data_list, ds["time"][:])

        # Store other coordinate data
        for coord in coord_names
            push!(coords_data_dict[coord], ds[coord][:])
        end

        close(ds)
    end

    # Pair variable data with time data and sort by time
    sorted_pairs = sort(collect(zip(time_data_list, var_data_list)); by=x -> x[1])
    sorted_time_data = [pair[1] for pair in sorted_pairs]
    sorted_var_data = [pair[2] for pair in sorted_pairs]

    # Concatenate sorted data
    concatenated_data_dict = Dict(
        variable_name => vcat(sorted_var_data...), "time" => vcat(sorted_time_data...)
    )

    # Concatenate coordinate data and add to the dictionary
    for coord in coord_names
        concatenated_data_dict[coord] = vcat(coords_data_dict[coord]...)
    end

    return concatenated_data_dict
end
```

```{julia}
#open files and extract data into NCdataset and/or Dictionaries 
data_dict_temp = open_mfdataset(["data/raw/2m_temperature_2000.nc", "data/raw/2m_temperature_2001.nc", "data/raw/2m_temperature_2002.nc","data/raw/2m_temperature_2003.nc","data/raw/2m_temperature_2004.nc","data/raw/2m_temperature_2005.nc","data/raw/2m_temperature_2006.nc","data/raw/2m_temperature_2007.nc","data/raw/2m_temperature_2008.nc","data/raw/2m_temperature_2009.nc","data/raw/2m_temperature_2010.nc","data/raw/2m_temperature_2011.nc","data/raw/2m_temperature_2012.nc","data/raw/2m_temperature_2013.nc","data/raw/2m_temperature_2014.nc","data/raw/2m_temperature_2015.nc","data/raw/2m_temperature_2016.nc","data/raw/2m_temperature_2017.nc","data/raw/2m_temperature_2018.nc","data/raw/2m_temperature_2019.nc","data/raw/2m_temperature_2020.nc","data/raw/2m_temperature_2021.nc","data/raw/2m_temperature_2022.nc"], "t2m")

# println(length(data_dict_temp[:"time"]))
# println(length(data_dict_temp[:"t2m"]))

temp_ds = NCDataset("data/raw/2m_temperature_2019.nc")

data_dict_press = open_mfdataset(["data/raw/500hPa_geopotential_2000.nc", "data/raw/500hPa_geopotential_2001.nc", "data/raw/500hPa_geopotential_2002.nc","data/raw/500hPa_geopotential_2003.nc","data/raw/500hPa_geopotential_2004.nc","data/raw/500hPa_geopotential_2005.nc","data/raw/500hPa_geopotential_2006.nc","data/raw/500hPa_geopotential_2007.nc","data/raw/500hPa_geopotential_2008.nc","data/raw/500hPa_geopotential_2009.nc","data/raw/500hPa_geopotential_2010.nc","data/raw/500hPa_geopotential_2011.nc","data/raw/500hPa_geopotential_2012.nc","data/raw/500hPa_geopotential_2013.nc","data/raw/500hPa_geopotential_2014.nc","data/raw/500hPa_geopotential_2015.nc","data/raw/500hPa_geopotential_2016.nc","data/raw/500hPa_geopotential_2017.nc","data/raw/500hPa_geopotential_2018.nc","data/raw/500hPa_geopotential_2019.nc","data/raw/500hPa_geopotential_2020.nc","data/raw/500hPa_geopotential_2021.nc","data/raw/500hPa_geopotential_2022.nc"], "z");
```

```{julia}
#reshape and correct data 

dataTemp = reshape(data_dict_temp[:"t2m"],13, 13,length(data_dict_temp[:"time"]))
dataPress = reshape(data_dict_press[:"z"],13, 13,length(data_dict_press[:"time"]))
# println(length(dataPress[:,:,1]))

temp_lat = reverse(temp_ds[:"latitude"])
temp_lon = temp_ds[:"longitude"]
temp = reverse(dataTemp; dims=2)
# println(length(data_dict_temp[:"time"]))


temp_time = data_dict_temp[:"time"]
press_time = data_dict_press[:"time"]

# #check the form of the data 
temp_data_plot = heatmap(
temp_lon,
temp_lat,
dataTemp[:, :, 1]';
xlabel="Longitude",
ylabel="Latitude",
title="Temperature - hour 1"
)

display(temp_data_plot)

press_data_plot = heatmap(
temp_lon,
temp_lat,
dataPress[:, :, 1]';
xlabel="Longitude",
ylabel="Latitude",
title="Pressure - hour 1"
)

```



plots of temperature and pressure to make sure the data looks right. It appears there is less gradient in the pressure data compared to the temperature across the land ocean boundary. 



```{julia}
#average by day to match the precip data 
temp_byDay = reshape(dataTemp, length(temp_lat), length(temp_lat), 24, 8401)
daily_mean = mean(temp_byDay, dims=3)
temp_byDay = reshape(daily_mean,length(temp_lat),length(temp_lat),8401)

#same for pressure
press_byDay = reshape(dataPress, length(temp_lat), length(temp_lat), 24, 8401)
daily_mean_press = mean(press_byDay, dims=3)
press_byDay = reshape(daily_mean_press,length(temp_lat),length(temp_lat),8401);
```


```{julia}
#check time and convert to Date data type 
@assert temp_time == press_time
time1 = Dates.Date.(temp_time)
time_p = Dates.Date.(precip_time_cut);
```

```{julia}
idx_partition = findfirst(time_p .== time_p[end] - Dates.Day(5544))

#split the data 150 days from end of data just for one year of data, may need to create a different partition later 

train_idx = 1:idx_partition
test_idx = (idx_partition+1):length(time_p)
precip_train = precip[:, :, train_idx]
precip_test = precip[:, :, test_idx]
temp_train = temp_byDay[:, :, train_idx]
temp_test = temp_byDay[:, :, test_idx]
press_train = press_byDay[:, :, train_idx]
press_test = press_byDay[:, :, test_idx];

```

```{julia}
function preprocess(temp::Array{T,3}, temp_ref::Array{T,3})::AbstractMatrix where {T}
    n_lon, n_lat, n_t = size(temp)
    climatology = mean(temp_ref; dims=3)
    temp_anom = temp .- climatology
    # reshape to 2D
    temp_anom = reshape(temp_anom, n_lon * n_lat, n_t)
    # strip the units
    return temp_anom
end
```

```{julia}
n_lon, n_lat, n_t = size(temp)
temp_mat_train = preprocess(temp_train, temp_train)
temp_mat_test = preprocess(temp_test, temp_train)
press_mat_train = preprocess(press_train, press_train)
press_mat_test = preprocess(press_test, press_train)

data_train = vcat(temp_mat_train,press_mat_train);
```

```{julia}
pca_model = fit(PCA, press_mat_train; maxoutdim=25, pratio=0.999);
```

```{julia}
p1 = plot(
principalvars(pca_model) / var(pca_model);
xlabel="# of PCs",
ylabel="Fraction of Variance Explained",
xticks = 0:2:25,
label=false,
title="Variance Explained"
)
p2 = plot(
cumsum(principalvars(pca_model)) / var(pca_model);
xlabel="# of PCs",
ylabel="Fraction of Variance Explained",
xticks = 0:2:25,
label=false,
title="Cumulative Variance Explained Plot"
)
plot(p1, p2; layout=(1, 2), size=(800, 400))

```

Plots of the variance explained and cumulative variance for each aditional PCA axis. 

```{julia}
p = []
for i in 1:3
pc = projection(pca_model)[:, i]
pc = reshape(pc, n_lat, n_lon)'
pi = heatmap(
temp_lon,
temp_lat,
pc;
xlabel="Longitude",
ylabel="Latitude",
title="PC $i",
aspect_ratio=:equal,
cmap=:PuOr
)
push!(p, pi)
end
plot(p...; layout=(1, 3), size=(1500, 600))

```

Plot of the spatial dependences of the PCA axes. 

```{julia}
pc_ts = predict(pca_model, press_mat_train)
day_of_year = Dates.dayofyear.(time1)
p = []
for i in 1:3
pi = scatter(
day_of_year,
pc_ts[i, :];
xlabel="Day of Year",
ylabel="PC $i",
title="PC $i",
label=false,
alpha=0.3,
color=:gray
)
push!(p, pi)
end
#plot(p...; layout=(1, 3), size=(1500, 600))
```

```{julia}
avg_precip = [mean(skipmissing(precip_train[:, :, t])) for t in 1:size(precip_train, 3)]
avg_precip = replace(avg_precip, NaN => 0)
p1 = scatter(
pc_ts[2, :],
pc_ts[3, :];
zcolor=avg_precip,
xlabel="PC 2",
ylabel="PC 3",
markersize=3,
clims=(0, 2.75),
title="All Days",
label=false
)
p2_idx = findall(avg_precip .> quantile(avg_precip, 0.98))
p2 = scatter(
pc_ts[2, p2_idx],
pc_ts[3, p2_idx];
zcolor=avg_precip[p2_idx],
xlabel="PC 2",
ylabel="PC 3",
markersize=5,
clims=(0, 2.75),
title="High Pressure Days",
label=false
)
plot(p1, p2; size=(1000, 400), link=:both)

```

principal components plotted against eachother, no clear trend in the plots.

```{julia}
function euclidean_distance(x::AbstractVector, y::AbstractVector)::AbstractFloat
    return sqrt(sum((x .- y) .^ 2))
end
function nsmallest(x::AbstractVector, n::Int)::Vector{Int}
    idx = sortperm(x)
    return idx[1:n]
end
function knn(X::AbstractMatrix, X_i::AbstractVector, K::Int)::Tuple{Int,AbstractVector}
# calculate the distances between X_i and each row of X
    dist = [euclidean_distance(X_i, X[j, :]) for j in 1:size(X, 1)]
    idx = nsmallest(dist, K)
    w = 1 ./ dist[idx]
    w ./= sum(w)
    idx_sample = sample(idx, Weights(w))
    return (idx_sample, vec(X[idx_sample, :]))
end
```

```{julia}
function predict_knn(temp_train, temp_test, precip_train; n_pca::Int)
    X_train = preprocess(temp_train, temp_train)
    X_test = preprocess(temp_test, temp_train)
    # fit the PCA model to the training data
    pca_model = fit(PCA, X_train; maxoutdim=n_pca)
    # project the test data onto the PCA basis
    train_embedded = predict(pca_model, X_train)
    test_embedded = predict(pca_model, X_test)
    # use the `knn` function for each point in the test data
    precip_pred = map(1:size(X_test, 2)) do i
    idx, _ = knn(train_embedded', test_embedded[:, i], 2)
    precip_train[:, :, idx]
end
# return a matrix of predictions
return precip_pred
end
```

```{julia}
    #t_sample = rand(1:size(press_test, 3), 3)
    t_sample = [1200,500,2300]
    precip_pred = predict_knn(press_train, press_test[:, :, t_sample], precip_train; n_pca=3)
    p = map(eachindex(t_sample)) do ti
    t = t_sample[ti]
    y_pred = precip_pred[ti]'
    y_actual = precip_test[:, :, t]'
    
    cmax = max(maximum(skipmissing(y_pred)), maximum(skipmissing(y_actual)))
    #cmax = ustrip(u"mm", cmax)
    # println(size(y_pred))
    p1 = heatmap(
    precip_lon,
    precip_lat,
    y_pred;
    xlabel="Longitude",
    ylabel="Latitude",
    title="Predicted",
    aspect_ratio=:equal,
    clims=(0, cmax)
    )
    
    
    y = round(abs(sum(skipmissing(y_pred - y_actual))/576); digits=3)

    p2 = heatmap(
    precip_lon,
    precip_lat,
    y_actual;
    xlabel="Longitude",
    ylabel="Latitude",
    title="Actual\nMAE="*"$y",
    aspect_ratio=:equal,
    clims=(0, cmax)
    )

    plot(p1, p2; layout=(2, 1), size=(1000, 400))
    end

    plot(p...; layout=(2, 3), size=(800, 400))

```

```{julia}
precip_pred_QQ = predict_knn(press_train, press_test[:, :, [1800,500,2100,750,100,1300]], precip_train; n_pca=3);
```

```{julia}
#quantile-quantile mapping
y_pred_QQ = precip_pred_QQ[4]'
#display(y_pred_QQ)
y_actual_QQ = precip_test[:, :, 750]
y_pred_QQ = reshape(y_pred_QQ,length(precip_lon)*length(precip_lat))
y_pred_corrected = zeros((length(precip_lon), length(precip_lat)))

for i in skipmissing(y_pred_QQ)
        #Chat GPT generated code to calcute the quantile of a datapoint
        point_quant = searchsortedfirst(sort(y_pred_QQ), i) / length(y_pred_QQ)
        #calculate the corresponding value at the quantile of the predicted datapoint
        quant_actual = quantile(skipmissing(y_actual_QQ),point_quant)
        idx = findfirst(x -> x == i, skipmissing(y_pred_QQ))
        #replace value with the mapped value
        y_pred_corrected[idx] = quant_actual

end
y_pred_corrected = reshape(y_pred_corrected,length(precip_lon), length(precip_lat));
```


```{julia}
y_pred_QQ = precip_pred_QQ[4]'

#mean absolute error
y = round(abs(sum(skipmissing(y_pred_corrected - y_actual_QQ))/576); digits=3)
x = round(abs(sum(skipmissing(y_pred_QQ - y_actual_QQ))/576);digits=3)

plot1 = heatmap(
precip_lon,
precip_lat,
y_pred_corrected;
xlabel="Longitude",
ylabel="Latitude",
title="PCA-KNN corrected:\nMAE="*"$y",
xticks = 250:5:270,
aspect_ratio=:equal,
)

plot2 = heatmap(
precip_lon,
precip_lat,
y_pred_QQ;
xlabel="Longitude",
ylabel="Latitude",
title="PCA-KNN predicted: \nMAE="*"$x",
xticks = 250:5:270,
aspect_ratio=:equal,
)

plot3 = heatmap(
precip_lon,
precip_lat,
y_actual_QQ';
xlabel="Longitude",
ylabel="Latitude",
title="Actual",
xticks = 250:5:270,
aspect_ratio=:equal,
)

plot(plot1,plot2,plot3; layout=(1, 3),size=(800, 400))

```

## executive summary

For my project I decided to attempt to use pressure data retrieved from the ERA5 reanalysis dataset and NEXRAD precipitation data to generate precipitation fields over south east Texas. Twenty years of pressure data were used in this analysis, from 2000 to 2022. To accomplish this I chose to use Principal component analysis to reduce the dimensions, and then a K nearest neighbor algorithm to generate predictions by sampling from the precipitation data set. Looking to improve the generated precipitation fields, I implemented a quantile-quantile mapping scheme. Where the output from the PCA-KNN is adjusted by matching corresponding quartiles from the actual observations. This would help the prediction better match the actual precipitation fields. 

Principal Component Analysis is a technique that is used to analyze high dimensional systems, where the high dimensionality would otherwise be less easy to interpret and preform analysis on. It is accomplished by transforming the data into a new coordinate system that consists of fewer axes, or dimensions,  than the original dataset. These new coordinates, called principal components, are chosen such that they explain the most variance in the predictors. Thus retaining the explaining power of the original variables.K-Nearest-Neighbors, is a prediction technique that assumes the value of a piece of data is related to the values that are near to it. A prediction is made by averaging the neighbors or randomly sampling from them. The second method I have chosen to implement is quantile-quantile mapping. This method is used to eliminate biases in data. These biases can come from coarse data, or poor model design. It is preformed by mapping the predicted values quantile to the quantiles of the observed data. Essentially trying to have the predicted data's CDF equal the observed data's CDF. 


## methods
Data: The first decision to be made was how much data to include in the analysis. As I am looking to make predictions about the daily precipitation from regional pressure data, a longer period to capure the full climatology is appropriate. For this project I chose 20 years of data from the ERA5 reanalysis dataset. The pressure data comes as hourly data, where the precipitaion fields are come as daily. To make these two datasets match, I did a daily mean of all of the hourly pressure data. 

PCA: The next step was to split the data into a training set and a testing set. Two thirds went to the training sets, and the remaining third went to the testing sets. When conducting PCA analysis it is important to standardize the data so the size of one variable compared to another doesnt dominate the variance. To accomplish this the mean was subtracted from the data. To determine how many PCA axes was nessesary a plot of the variance explained per axis was made, from this I determined three PCA axes was sufficient. 

KNN: In the K-Nearest-Neighbors algorithm, I chose to use a simple euclidean distance to calculate the nearest neighbors. This is appropriate, as our main variable in the PCA analysis is pressure which is a smooth and continuous variable. To make the prediction, a random weighted sample of the neighbors was made. The samples were weighted by distance, so the closer neighbors were more likely to be chosen. To determine the ammount of neighbors to sample I experimented by changing the number of neighbors and visually assesing the impact on the predictions. This led me to believe that two neighbors was a decent choice.

Q-Q Mapping: To implement my quantile mapping correction, I went by simply comparing the quantiles of the predicted and observed precipitation fields. To validate the effectiveness of my model I did a simple mean absolute error calculation between the corrected and the uncoreccted predictions vs the observed precipitation field. The goal of Q-Q mapping is to adjust the CDF of the predicted values to the CDF of the observed values. A better way to validate would be to calculate an empierical CDF for each of the predicted precipitation fields and plot them against the observed ECDF. This would tell us how sucessfull we were in adjusting the predicted CDF to match the observed. I tried to implement this but could not get past some errors in my code. 


## model comparison 

Plotting the PCA-KNN predictions against the actual precipitation fields and just visually we can see that there are some large discrepencies. This is confirmed by a mean abosolute error value. It is worth noting that the MAE metric does not take into account spaitial allignment of the prediction and the observed, only the total intensities. Another observation is that the PCA-KNN analysis appears to underestimates the precipitation fields in most cases. It also doesn't capture the spatial organization of the field very well either. These issues could be symptom of the number of nearest neighbors being to low. Another issue with the model is that only pressure is taken into account. Taking into account more climate variables such as temperature and water vapor would likely help the overall prediction. 

The second model, with the addition of a Quantile-Quantile mapping step after the PCA-KNN analysis, seems to preform slightly better. It is most improved in its ability to match the total ammount of rainfall on the grid. This makes sense as we are directly mapping from the precipitation values of each day. Though, it does have a tendency to way over or under predict the ammount of precipitation. Where if the uncorrected prediction field has more precipitation coverage over the grid, and the observed has high intensities. It will map to those higher intensity quantiles more often. The dependence on the agreement of the underlying PCA-KNN model is a major limitation. Preforming a Q-Q mapping analysis on just the pressure or temperature and the precipitation data could make it more less suseptible to these swings. This model preformed just about as poorly as the uncorrected PCA-KNN in its ability to predict the spatial organization of the observed fields. This is expected as the predictions are built off of the underlying PCA-KNN model. 

## conclusion
I explored the prediction power of a PCA-KNN model, compared with a similar model with an extra correction step from a Quantile-Quantile mapping scheme. I found that the PCA-KNN model tends to under predict the ammount of rainfall. This result may be attributed to the lack of nearest neighbors considered and not including more climate variables. The addition of the Quantile-Quantile mapping helped improve the model slightly, representing the total rainfall more accurately. Though it did seem more suceptible to extreme swings in its predictions. A result of the dependence on the model it is trying to correct.  